{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title: computing bias\n",
    "description:  computing bias homework\n",
    "type: issues \n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack 1\n",
    "### Biased System:\n",
    "A biased system I’ve read about is an AI-powered facial recognition system used by law enforcement. These systems have been shown to have higher error rates for people of color, particularly Black individuals. The system was trained on a dataset that was predominantly made up of lighter-skinned faces.\n",
    "\n",
    "### Type of Bias:\n",
    "This is an example of *Pre-existing Social Bias*. The facial recognition technology was trained on data that reflects societal inequalities, where there is a historical underrepresentation of darker-skinned individuals in the datasets. As a result, the system unfairly discriminates against people of color.\n",
    "\n",
    "### Suggestion to Fix the Bias:\n",
    "To reduce this bias, the dataset used to train the system should be more diverse and representative of all skin tones. Additionally, bias detection tools could be incorporated to regularly audit the system for fairness and accuracy across different demographic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn hack 2\n",
    "### Biased System:\n",
    "In the financial industry, an AI system used to approve loan applications unintentionally favors male applicants over female applicants because it was trained on past loan approval data, which reflected gender biases.\n",
    "\n",
    "### Type of Bias:\n",
    "This is an example of *Pre-existing Social Bias*, as the system has learned from past data that reflects gender imbalances in loan approvals.\n",
    "\n",
    "### Two Ways to Mitigate This Bias:\n",
    "\n",
    "1. **Diversifying the Training Data:**\n",
    "   The AI system should be retrained using a more diverse and balanced dataset, where both male and female applicants are represented equally. This will help the system learn from fairer data and reduce the gender bias.\n",
    "\n",
    "2. **Implementing Bias Detection and Monitoring:**\n",
    "   Regular audits of the AI system should be performed to detect any bias in its decision-making process. This can be done by checking approval rates across gender, ethnicity, and other demographic factors to ensure that the system is not favoring one group over another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework hack\n",
    "### System:\n",
    "I use a video streaming platform that recommends movies and shows based on my watch history. The system suggests content that is similar to what I have already watched, often focusing on a specific genre like action or drama.\n",
    "\n",
    "### Bias:\n",
    "The system is biased because it limits my content choices to what I’ve already watched, which means it reinforces my existing preferences and doesn't offer a diverse range of options. This is an example of *Emergent Social Bias*, as the system evolves over time based on my watching behavior and inadvertently narrows my options.\n",
    "\n",
    "### Solution:\n",
    "To reduce this bias, the platform could introduce a feature that recommends content from a wider variety of genres, including those I don’t typically watch. This would encourage exposure to new types of content and help break the pattern of bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit\n",
    "![www](https://github.com/collin12396/newCollinCSP_2025/raw/main/images/1111.png)\n",
    "\n",
    "### Notes:\n",
    "### 1. **Pre-existing Social Bias**\n",
    "**System:** Social media platform recommendation algorithms  \n",
    "**Bias Explanation:** The recommendation algorithm often favors content that aligns with users' historical interests, which can perpetuate societal biases such as gender or racial stereotyping. For example, women may be recommended more beauty-related content, while men are recommended more tech-focused content, reinforcing stereotypes.  \n",
    "**Impact on Lives:** This bias can reinforce existing stereotypes, limit exposure to diverse viewpoints, and create echo chambers, especially in areas like career development, body image, and interests.  \n",
    "**Solution:** Introduce more diverse training data that includes content from a wide variety of gender and cultural perspectives. The algorithm can be adjusted to recommend content that encourages exploration beyond one's typical interests, providing more balanced exposure.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Technical Bias**\n",
    "**System:** AI-based hiring tool  \n",
    "**Bias Explanation:** An AI hiring system that has been trained on historical hiring data may unintentionally favor male candidates due to a lack of gender balance in the past data. The system might favor male applicants because it has learned from past hiring patterns that reflect societal bias.  \n",
    "**Impact on Lives:** This technical bias can limit opportunities for women or other underrepresented groups in fields where they have historically been excluded, such as in STEM.  \n",
    "**Solution:** Modify the system to ensure that the training data is more balanced, and include algorithms that specifically check for gender parity. Regular audits and adjustments can help to ensure fairness in hiring recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Emergent Social Bias**\n",
    "**System:** Online multiplayer video game  \n",
    "**Bias Explanation:** In an online multiplayer game, players may create in-game content or make decisions that reflect biases based on group behavior. For example, certain players might engage in toxic behavior, excluding others based on their race, gender, or playstyle. This results in an emergent social bias where the in-game culture becomes unwelcoming for certain groups.  \n",
    "**Impact on Lives:** This bias can create an unwelcoming or hostile environment for players who are minorities or those who don't fit into a dominant group dynamic, discouraging participation and enjoyment.  \n",
    "**Solution:** Introduce moderation tools and mechanisms to address toxic behavior, such as reporting features or automated systems that detect and punish discriminatory language. Game developers can also actively promote positive player behavior by recognizing inclusivity and implementing features that encourage cooperative play across diverse groups.\n",
    "\n",
    "---\n",
    "\n",
    "These examples reflect how different types of biases can emerge in systems and how they impact our daily lives. The suggested solutions can help reduce or mitigate these biases, leading to more fair, inclusive, and effective systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
