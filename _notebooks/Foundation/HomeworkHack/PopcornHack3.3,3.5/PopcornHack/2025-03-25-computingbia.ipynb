{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "layout: post\n",
    "title: computing bias\n",
    "description:  computing bias homework\n",
    "type: issues \n",
    "comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack 1\n",
    "### Biased System:\n",
    "A biased system I’ve read about is an AI-powered facial recognition system used by law enforcement. These systems have been shown to have higher error rates for people of color, particularly Black individuals. The system was trained on a dataset that was predominantly made up of lighter-skinned faces.\n",
    "\n",
    "### Type of Bias:\n",
    "This is an example of *Pre-existing Social Bias*. The facial recognition technology was trained on data that reflects societal inequalities, where there is a historical underrepresentation of darker-skinned individuals in the datasets. As a result, the system unfairly discriminates against people of color.\n",
    "\n",
    "### Suggestion to Fix the Bias:\n",
    "To reduce this bias, the dataset used to train the system should be more diverse and representative of all skin tones. Additionally, bias detection tools could be incorporated to regularly audit the system for fairness and accuracy across different demographic groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn hack 2\n",
    "### Biased System:\n",
    "In the financial industry, an AI system used to approve loan applications unintentionally favors male applicants over female applicants because it was trained on past loan approval data, which reflected gender biases.\n",
    "\n",
    "### Type of Bias:\n",
    "This is an example of *Pre-existing Social Bias*, as the system has learned from past data that reflects gender imbalances in loan approvals.\n",
    "\n",
    "### Two Ways to Mitigate This Bias:\n",
    "\n",
    "1. **Diversifying the Training Data:**\n",
    "   The AI system should be retrained using a more diverse and balanced dataset, where both male and female applicants are represented equally. This will help the system learn from fairer data and reduce the gender bias.\n",
    "\n",
    "2. **Implementing Bias Detection and Monitoring:**\n",
    "   Regular audits of the AI system should be performed to detect any bias in its decision-making process. This can be done by checking approval rates across gender, ethnicity, and other demographic factors to ensure that the system is not favoring one group over another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework hack\n",
    "### System:\n",
    "I use a video streaming platform that recommends movies and shows based on my watch history. The system suggests content that is similar to what I have already watched, often focusing on a specific genre like action or drama.\n",
    "\n",
    "### Bias:\n",
    "The system is biased because it limits my content choices to what I’ve already watched, which means it reinforces my existing preferences and doesn't offer a diverse range of options. This is an example of *Emergent Social Bias*, as the system evolves over time based on my watching behavior and inadvertently narrows my options.\n",
    "\n",
    "### Solution:\n",
    "To reduce this bias, the platform could introduce a feature that recommends content from a wider variety of genres, including those I don’t typically watch. This would encourage exposure to new types of content and help break the pattern of bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
